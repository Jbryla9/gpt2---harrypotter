This project involved training a GPT-2-like language model architecture on the entire Harry Potter series to explore the effects of overfitting and to understand how GPT-like models are built. Inspired by Andrew Karpathy's lectures on large language models, I implemented a causal self-attention mechanism and multi-layer perceptrons to create a transformer-based model. I analyzed the generated outputs to understand how the model mimics the writing style and structure of the original books and it was able to generate text resembling the Harry Potter universe, showcasing its capacity to learn and reproduce language.

Model used in eval_model.py could be find here: https://drive.google.com/file/d/1FXlycyh5BN1rAurXGWjwS6rr0NVh49-_/view?usp=drive_link

Text generated by the model after providing input: "He couldn’t kill that little boy. No one knows why, or how, but they’re saying that when he couldn’t kill Harry Potter, Voldemort’s power somehow broke — and that’s why he’s gone":

*He couldn’t kill that little boy. No one knows why, or how, but they’re saying that when he couldn’t kill Harry Potter, Voldemort’s power somehow broke — and that’s why he’s gone should find himself! The Death Eaters had been murdered to you – but he is not it, and Voldemort,his
drout of his mother; for it, but she pulled her.
“Harry didn’s what?’s got Greghed
 “What should do you’t know … you didn”
“Hermione gasped. ‘ mentioned that I
And I don”
disappear it didn’re going to do you’ve made
reach his life, ‘I“I know, and Hermione said. He didn’ve been loved sure I’ you leaving*
