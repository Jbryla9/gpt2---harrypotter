This project involved training a GPT-2-like language model architecture on the entire Harry Potter series to explore the effects of overfitting and to understand how GPT-like models are built. Inspired by Andrew Karpathy's lectures on large language models, I implemented a causal self-attention mechanism and multi-layer perceptrons to create a transformer-based model. I analyzed the generated outputs to understand how the model mimics the writing style and structure of the original books and it was able to generate text resembling the Harry Potter universe, showcasing its capacity to learn and reproduce language.
